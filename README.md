# ğŸ† Multimodal RAG System

### Offline Â· Cross-Modal Â· Hardware-Aware Â· Makeathon 2026

An **offline-capable Multimodal Retrieval-Augmented Generation (RAG)** system that ingests **PDF, DOCX, PPTX, Images, and Audio**, performs **hybrid search (Milvus + Tantivy)**, and generates **grounded answers with rich, cross-modal citations**.

Designed to run on a **4GB VRAM GPU (RTX 3050)**.
No cloud APIs. No external inference services.

---

## ğŸ‘¥ Team â€” Makeathon 2026

**Team Leader:** Praveen Ram<br>
**Architecture Designed By:** Praveen Ram

**Team Members:**

* Praveen Ram
* Sachin Aadithya. V
* Abishek Roshan KMS
* Murugan
* Sahana N
* Deepa L

ğŸ’» Developed and tested on Sachinâ€™s laptop (RTX 3050 â€“ 4GB VRAM)

---

# âœ¨ What Makes This Different?

Most â€œmultimodalâ€ systems convert everything to text and call it RAG.

This system uses **native embedding spaces per modality**:

* **Text & Audio** â†’ BGE-M3 (1024-dim)
* **Images** â†’ CLIP ViT-B/32 (512-dim)
* **Audio Transcription** â†’ Whisper (word-level timestamps)
* **Hybrid Retrieval** â†’ Milvus (Vector) + Tantivy (BM25F)
* **Cross-Modal Link Graph** â†’ SQLite
* **Speculative Decoding** â†’ Llama 3.2 (3B + 1B)

True multimodal retrieval â€” not OCR-only search.

---

# ğŸ§  Core Capabilities

* ğŸ“„ PDF / DOCX / PPTX ingestion
* ğŸ–¼ Image extraction + visual embeddings
* ğŸ§ Audio transcription with timestamps
* ğŸ” Hybrid search (semantic + keyword)
* ğŸ”— Cross-modal linking (text â†” image â†” audio)
* ğŸ“Œ Grounded answers with structured citations
* ğŸ“¤ Export (DOCX / XLSX / PPTX / CSV)
* ğŸ§  Session memory
* ğŸ”’ Fully offline execution
* âš™ï¸ 4GB VRAM-aware architecture

---

# ğŸ— Architecture Overview

### 1ï¸âƒ£ Ingestion Engine

* Structured chunking (500 tokens, 50 overlap)
* Rich metadata per chunk
* Page-level and timestamp-level traceability

### 2ï¸âƒ£ Dual Index System

**Milvus (Vector Search)**

* `text_chunks` â†’ 1024-dim
* `image_chunks` â†’ 512-dim
* `audio_chunks` â†’ 1024-dim

**Tantivy (BM25F Sparse Search)**

* Incremental indexing
* No full rebuild on new uploads

### 3ï¸âƒ£ Hybrid Retrieval

* Parallel ANN + BM25
* Reciprocal Rank Fusion (RRF)
* Cross-encoder reranking
* Modality diversification
* Cross-modal enrichment

### 4ï¸âƒ£ LLM Engine

* Llama 3.2 3B (main model)
* Llama 3.2 1B (draft model)
* Speculative decoding (80â€“120 tokens/sec)
* Citation-aware prompting

### 5ï¸âƒ£ Citation Engine

* Page-level text citations
* Image bounding-box references
* Timestamped audio citations
* Cross-modal link display

---

# ğŸš€ Quick Start (Windows)

### 1ï¸âƒ£ Activate Environment

```powershell
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\.venv\Scripts\Activate.ps1
```

---

### 2ï¸âƒ£ Start Milvus (Docker Required)

```powershell
docker run -d --name milvus-standalone -p 19530:19530 -p 9091:9091 milvusdb/milvus:v2.4.5
```

Wait ~20 seconds.

---

### 3ï¸âƒ£ Install Dependencies

```powershell
pip install -r requirements.txt
```

Verify:

```powershell
python scripts/health_check.py
```

---

# â–¶ Running the System

### Backend (FastAPI)

```powershell
python api/main.py
```

Runs on:
`http://localhost:8000`

---

### Frontend (Gradio UI)

```powershell
python frontend/app.py
```

Runs on:
`http://localhost:7860`

---

# ğŸ“‚ Project Structure

```
api/         â†’ FastAPI backend
frontend/    â†’ Gradio UI
modules/     â†’ Core logic (Ingestion, Retrieval, LLM, Citation, Export)
scripts/     â†’ Utilities (Health, Benchmark, Seeding)
tests/       â†’ Unit + integration tests
data/        â†’ Uploads, DB, Vector Index
models/      â†’ GGUF model files
```

---

# ğŸ§ª Testing

Run all tests:

```powershell
pytest
```

Run end-to-end test:

```powershell
pytest tests/test_e2e_pipeline.py
```

---

# ğŸ¬ Demo Highlights (Makeathon)

### ğŸ”¹ Text â†’ Image Retrieval

> â€œShow me the Q3 revenue chartâ€

Retrieves the correct image via CLIP semantic similarity â€” even without matching OCR text.

---

### ğŸ”¹ Timestamp Navigation

> â€œWhat was discussed at 14 minutes?â€

Returns:

* Audio segment
* Transcript
* Linked documents

---

### ğŸ”¹ Cross-Format Evidence

> â€œFind all evidence about budget approvalâ€

Returns:

* PDF paragraph
* Signed image
* Audio confirmation
  All cross-linked.

---

# ğŸ›  Utilities

Health Check:

```powershell
python scripts/health_check.py
```

Benchmark:

```powershell
python scripts/benchmark.py
```

Seed Demo Data:

```powershell
python scripts/seed_demo_data.py
```

---

# ğŸ”’ Fully Offline

* No OpenAI API
* No cloud inference
* No external embedding services
* No internet required during runtime

Designed for constrained GPU environments.

---

# ğŸ Conclusion

A production-ready, hardware-aware, fully offline multimodal RAG system with:

* Native modality embeddings
* Hybrid retrieval
* Cross-modal linking
* Transparent citations
* Export-ready structured outputs

Built for Makeathon 2026.
Engineered for real-world constraints.




